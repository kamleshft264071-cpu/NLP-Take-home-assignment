{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e60a674c-e3ae-4f44-86ff-aeb8e7f445c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\kamle\\anaconda3\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\kamle\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\kamle\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\kamle\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: nltk in c:\\users\\kamle\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\kamle\\anaconda3\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\kamle\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kamle\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kamle\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kamle\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: click in c:\\users\\kamle\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\kamle\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\kamle\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kamle\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\kamle\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kamle\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kamle\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# Install the missing libraries directly from Jupyter\n",
    "!pip install textblob beautifulsoup4 pandas numpy nltk openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220310c9-df10-4b19-8765-a95f64ae2355",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3877f63e-2e81-4103-be04-03ba09f9b77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e536016-f4e4-47e6-b035-258d4c9aacf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 2.2.2\n",
      "Numpy version: 1.26.4\n",
      "NLTK version: 3.9.1\n",
      "Python version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kamle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kamle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 1. Import Libraries & Print Versions\n",
    "# ---------------------------------------------------------\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Numpy version: {np.__version__}\")\n",
    "print(f\"NLTK version: {nltk.__version__}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Download necessary NLTK data (run once)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4cabe99-1532-46b2-a3fd-b10a2313722c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded. Shape: (4846, 2)\n",
      "  Sentiment                                           Headline\n",
      "0   neutral  According to Gran , the company has no plans t...\n",
      "1   neutral  Technopolis plans to develop in stages an area...\n",
      "2  negative  The international electronic industry company ...\n",
      "3  positive  With the new production plant the company woul...\n",
      "4  positive  According to the company 's updated strategy f...\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 2. Load Data\n",
    "# ---------------------------------------------------------\n",
    "# The dataset appears to lack headers, so we added the columns manually in excel\n",
    "#data = pd.read_csv(\"/content/all-data.xlsx\")\n",
    "df = pd.read_excel('E:/NLP/Take_home assignment/all-data.xlsx')\n",
    "\n",
    "if len(df.columns) == 2:\n",
    "    df.columns = ['Sentiment', 'Headline']\n",
    "\n",
    "print(\"Data Loaded. Shape:\", df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "00f634b9-e67c-4f78-a501-345b387339df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to lowercase\n",
    "# Apply only to the 'Headline' column\n",
    "df['Headline'] = df['Headline'].str.lower()\n",
    "df['Sentiment'] = df['Sentiment'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b46d042d-82a3-47b0-81de-24066681fd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentiment                                           Headline\n",
      "0   neutral  according to gran , the company has no plans t...\n",
      "1   neutral  technopolis plans to develop in stages an area...\n",
      "2  negative  the international electronic industry company ...\n",
      "3  positive  with the new production plant the company woul...\n",
      "4  positive  according to the company 's updated strategy f...\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d88c91ce-34f6-41e8-a44e-616e26f66a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# g. Acronym Dictionary (added based on some observations from db)\n",
    "acronym_dict = {\n",
    "    \"lol\": \"laugh out loud\",\n",
    "    \"asap\": \"as soon as possible\",\n",
    "    \"fyi\": \"for your information\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"eur\": \"euro\",\n",
    "    \"usd\": \"dollar\",\n",
    "    \"mn\" : \"million\",\n",
    "    \"mln\" : \"million\",\n",
    "    \"$\" : \"dollar\",\n",
    "    \"%\" : \"percent\",\n",
    "    \"bn\" : \"billion\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eae59eb6-9c42-4456-a6cc-9e538619e2f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"a, about, above, after, again, against, ain, all, am, an, and, any, are, aren, aren't, as, at, be, because, been, before, being, below, between, both, but, by, can, couldn, couldn't, d, did, didn, didn't, do, does, doesn, doesn't, doing, don, don't, down, during, each, few, for, from, further, had, hadn, hadn't, has, hasn, hasn't, have, haven, haven't, having, he, he'd, he'll, her, here, hers, herself, he's, him, himself, his, how, i, i'd, if, i'll, i'm, in, into, is, isn, isn't, it, it'd, it'll, it's, its, itself, i've, just, ll, m, ma, me, mightn, mightn't, more, most, mustn, mustn't, my, myself, needn, needn't, no, nor, not, now, o, of, off, on, once, only, or, other, our, ours, ourselves, out, over, own, re, s, same, shan, shan't, she, she'd, she'll, she's, should, shouldn, shouldn't, should've, so, some, such, t, than, that, that'll, the, their, theirs, them, themselves, then, there, these, they, they'd, they'll, they're, they've, this, those, through, to, too, under, until, up, ve, very, was, wasn, wasn't, we, we'd, we'll, we're, were, weren, weren't, we've, what, when, where, which, while, who, whom, why, will, with, won, won't, wouldn, wouldn't, y, you, you'd, you'll, your, you're, yours, yourself, yourselves, you've\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stop words check\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "\", \".join(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b3380da-a17b-419e-8c5f-95c3ec2c4703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critical for Sentiment Analysis: Remove negation words from the stopword list\n",
    "# because \"not happy\" is very different from \"happy\".\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    negation_words = {\n",
    "    'no', 'not', 'nor', 'neither', 'none', 'never',\n",
    "    \"don't\", \"aren't\", \"won't\", \"didn't\", \"couldn't\",\n",
    "    \"doesn't\", \"mightn\", \"mightn't\", \"mustn't\",\n",
    "    \"needn\", \"against\", \"needn't\", \"wouldn't\"\n",
    "}\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5420bea3-b9fa-4b2a-aa7d-299a60481def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text cleaning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kamle\\AppData\\Local\\Temp\\ipykernel_15324\\2246807372.py:11: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Success! Data Cleaned ---\n",
      "                                            Headline  \\\n",
      "0  according to gran , the company has no plans t...   \n",
      "1  technopolis plans to develop in stages an area...   \n",
      "2  the international electronic industry company ...   \n",
      "3  with the new production plant the company woul...   \n",
      "4  according to the company 's updated strategy f...   \n",
      "\n",
      "                                    cleaned_headline  \n",
      "0  according gran company plans move production r...  \n",
      "1  technopolis plans develop in stages an area no...  \n",
      "2  international electronic industry company elco...  \n",
      "3  the new production plant the company would inc...  \n",
      "4  according company s updated strategy years bas...  \n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 4. Cleaning Function (Steps a-c,g,d)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return str(text)\n",
    "\n",
    "    # c. Remove HTML tags\n",
    "    # BeautifulSoup is the most robust method for this\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    \n",
    "    # b. Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # g. Replace Acronyms with words\n",
    "    # We do this before removing special characters to match the acronyms correctly\n",
    "    words = text.split()\n",
    "    # Replace if word (upper) matches dict key\n",
    "    words = [acronym_dict.get(word.lower(), word) for word in words]\n",
    "    text = \" \".join(words)\n",
    "\n",
    "    #a. Remove special characters, numbers, and extra whitespace\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "  \n",
    "    # e. Stopwords\n",
    "    tokens = text.split()\n",
    "    # Use 'my_stop_words' (the set), NOT 'stopwords' (the library)\n",
    "    tokens = [w for w in tokens if w in remove_stopwords(text)]\n",
    "    text = \" \".join(tokens)\n",
    "    return text\n",
    "\n",
    "\n",
    "#Apply changes\n",
    "\n",
    "print(\"Processing text cleaning...\")\n",
    "df['cleaned_headline'] = df['Headline'].apply(clean_text)\n",
    "\n",
    "print(\"\\n--- Success! Data Cleaned ---\")\n",
    "print(df[['Headline', 'cleaned_headline']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c0ec107a-a059-4fea-b0fc-140c69585534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying spellcheck to first 5 rows...\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 5. Spellcheck (Step f)\n",
    "# ---------------------------------------------------------\n",
    "# Note: Spellchecking is computationally expensive (O(n)). \n",
    "# For large datasets (4000+ rows), this can take several minutes.\n",
    "# We will define the function and apply it to a sample to demonstrate.\n",
    "\n",
    "def apply_spellcheck(text):\n",
    "    return str(TextBlob(text).correct())\n",
    "\n",
    "# Apply to a small sample to verify functionality without waiting too long\n",
    "print(\"\\nApplying spellcheck to first 5 rows...\")\n",
    "df['final_text_sample'] = df['cleaned_headline'].head(5).apply(apply_spellcheck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d19f526-8d7b-4a2e-832c-2140c25aee4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_headline'] = df['cleaned_headline'].apply(apply_spellcheck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b1a27995-5260-4efc-8515-27212402bf28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: 3876 rows\n",
      "Test Data: 970 rows\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. Prepare Data\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Convert the dataset into two lists: texts and labels\n",
    "X = df['cleaned_headline'].astype(str)\n",
    "labels = df['Sentiment']\n",
    "\n",
    "#Encode the sentiment\n",
    "label_mapping = {\"positive\": 1, \"negative\": 0, \"neutral\": 2}\n",
    "y = labels.map(label_mapping)\n",
    "\n",
    "# Split the data (80% Train, 20% Test)\n",
    "# It is best practice to split BEFORE vectorizing to prevent data leakage\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training Data: {X_train.shape[0]} rows\")\n",
    "print(f\"Test Data: {X_test.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fd19c619-15ed-4745-80c0-d44eb67e2fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Model A: Bag of Words (CountVectorizer)\n",
      "==================================================\n",
      "\n",
      "Feature Names (First 20 examples):\n",
      "['ab' 'abb' 'abc' 'ability' 'able' 'abloy' 'abp' 'abroad' 'ac' 'acacia'\n",
      " 'access' 'accessories' 'accident' 'accordance' 'according' 'account'\n",
      " 'accountant' 'accounted' 'accounting' 'accounts']\n",
      "\n",
      "Vectorization Array (First 5 rows):\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "--- BOW Results ---\n",
      "Accuracy: 0.7515\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.45      0.55       110\n",
      "           1       0.76      0.50      0.60       289\n",
      "           2       0.75      0.94      0.84       571\n",
      "\n",
      "    accuracy                           0.75       970\n",
      "   macro avg       0.74      0.63      0.66       970\n",
      "weighted avg       0.75      0.75      0.73       970\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 2. MODEL A: Bag of Words (CountVectorizer)\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Model A: Bag of Words (CountVectorizer)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Initialize Vectorizer\n",
    "# max_features=5000 keeps only the top 5000 most frequent words\n",
    "bow_vectorizer = CountVectorizer(max_features=5000) \n",
    "\n",
    "# Fit on Train, Transform both Train and Test\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
    "X_test_bow = bow_vectorizer.transform(X_test)\n",
    "\n",
    "# --- Print Vectorization Arrays as requested ---\n",
    "print(\"\\nFeature Names (First 20 examples):\")\n",
    "print(bow_vectorizer.get_feature_names_out()[:20])\n",
    "\n",
    "print(\"\\nVectorization Array (First 5 rows):\")\n",
    "print(X_train_bow.toarray()[:5]) \n",
    "\n",
    "# Train Random Forest\n",
    "rf_bow = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_bow.fit(X_train_bow, y_train)\n",
    "\n",
    "# Predict & Evaluate\n",
    "y_pred_bow = rf_bow.predict(X_test_bow)\n",
    "\n",
    "print(\"\\n--- BOW Results ---\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_bow):.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "61d9eaa3-822a-44b3-a246-e74574c99435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Model B: TF-IDF\n",
      "==================================================\n",
      "\n",
      "Feature Names (First 20 examples):\n",
      "['ab' 'abb' 'abc' 'ability' 'able' 'abloy' 'abp' 'abroad' 'ac' 'acacia'\n",
      " 'access' 'accessories' 'accident' 'accordance' 'according' 'account'\n",
      " 'accountant' 'accounted' 'accounting' 'accounts']\n",
      "\n",
      "Vectorization Array (First 5 rows):\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "--- TF-IDF Results ---\n",
      "Accuracy: 0.7557\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.43      0.54       110\n",
      "           1       0.77      0.48      0.59       289\n",
      "           2       0.75      0.96      0.84       571\n",
      "\n",
      "    accuracy                           0.76       970\n",
      "   macro avg       0.76      0.62      0.66       970\n",
      "weighted avg       0.76      0.76      0.73       970\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 3. MODEL B: TF-IDF Vectorizer\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Model B: TF-IDF\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Initialize Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Fit on Train, Transform both Train and Test\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# --- Print Vectorization Arrays as requested ---\n",
    "print(\"\\nFeature Names (First 20 examples):\")\n",
    "print(tfidf_vectorizer.get_feature_names_out()[:20])\n",
    "\n",
    "print(\"\\nVectorization Array (First 5 rows):\")\n",
    "print(X_train_tfidf.toarray()[:5])\n",
    "\n",
    "# Train Random Forest\n",
    "rf_tfidf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict & Evaluate\n",
    "y_pred_tfidf = rf_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "print(\"\\n--- TF-IDF Results ---\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_tfidf):.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33bd0d58-be57-44e4-a4c8-9b25d732d6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad8c16c1-e04a-4e52-9f9f-85a99325fe1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.12.3\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "print(bs4.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d51bf975-df51-42fa-9e7a-352766965d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.11\n",
      "1.5.1\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "print(spacy.__version__)\n",
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24ef9a0-9d3f-4d73-aa48-bf0f93881572",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
